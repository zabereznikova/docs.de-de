---
title: Bereitstellen des .NET für Apache Spark-Workers und für benutzerdefinierte Funktionsbinärdateien
description: Erfahren Sie, wie Sie den .NET für Apache Spark-Worker und benutzerdefinierte Funktionsbinärdateien bereitstellen.
ms.date: 10/09/2020
ms.topic: conceptual
ms.custom: mvc,how-to
ms.openlocfilehash: 19ecd4736baaf789a409229d35a6946c6021db45
ms.sourcegitcommit: 34968a61e9bac0f6be23ed6ffb837f52d2390c85
ms.translationtype: HT
ms.contentlocale: de-DE
ms.lasthandoff: 11/17/2020
ms.locfileid: "94688188"
---
# <a name="deploy-net-for-apache-spark-worker-and-user-defined-function-binaries"></a><span data-ttu-id="655d6-103">Bereitstellen des .NET für Apache Spark-Workers und für benutzerdefinierte Funktionsbinärdateien</span><span class="sxs-lookup"><span data-stu-id="655d6-103">Deploy .NET for Apache Spark worker and user-defined function binaries</span></span>

<span data-ttu-id="655d6-104">Diese Anleitung enthält allgemeine Anweisungen für die Bereitstellung des .NET für Apache Spark-Workers und für benutzerdefinierte Funktionsbinärdateien.</span><span class="sxs-lookup"><span data-stu-id="655d6-104">This how-to provides general instructions on how to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span> <span data-ttu-id="655d6-105">Sie lernen, welche Umgebungsvariablen eingerichtet werden müssen, sowie einige häufig verwendete Parameter zum Starten von Anwendungen mit `spark-submit`.</span><span class="sxs-lookup"><span data-stu-id="655d6-105">You learn which Environment Variables to set up, as well as some commonly used parameters for launching applications with `spark-submit`.</span></span>

## <a name="configurations"></a><span data-ttu-id="655d6-106">Konfigurationen</span><span class="sxs-lookup"><span data-stu-id="655d6-106">Configurations</span></span>

<span data-ttu-id="655d6-107">Die Konfigurationen zeigen die allgemeinen Umgebungsvariablen und Parametereinstellungen, um den .NET für Apache Spark-Worker und benutzerdefinierte Funktionsbinärdateien bereitzustellen.</span><span class="sxs-lookup"><span data-stu-id="655d6-107">Configurations show the general environment variables and parameters settings in order to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span>

### <a name="environment-variables"></a><span data-ttu-id="655d6-108">Umgebungsvariablen</span><span class="sxs-lookup"><span data-stu-id="655d6-108">Environment variables</span></span>

<span data-ttu-id="655d6-109">Bei der Bereitstellung von Workern und dem Schreiben von UDFs gibt es einige häufig verwendete Umgebungsvariablen, die Sie möglicherweise festlegen müssen:</span><span class="sxs-lookup"><span data-stu-id="655d6-109">When deploying workers and writing UDFs, there are a few commonly used environment variables that you may need to set:</span></span>

| <span data-ttu-id="655d6-110">Umgebungsvariable</span><span class="sxs-lookup"><span data-stu-id="655d6-110">Environment Variable</span></span>         | <span data-ttu-id="655d6-111">Beschreibung</span><span class="sxs-lookup"><span data-stu-id="655d6-111">Description</span></span>
| :--------------------------- | :----------
| <span data-ttu-id="655d6-112">DOTNET_WORKER_DIR</span><span class="sxs-lookup"><span data-stu-id="655d6-112">DOTNET_WORKER_DIR</span></span>            | <span data-ttu-id="655d6-113">Hierbei handelt es sich um den Pfad, in dem die <code>Microsoft.Spark.Worker</code>-Binärdatei generiert wurde.</span><span class="sxs-lookup"><span data-stu-id="655d6-113">Path where the <code>Microsoft.Spark.Worker</code> binary has been generated.</span></span></br><span data-ttu-id="655d6-114">Dieser wird vom Spark-Treiber verwendet und wird an die Spark-Executors weitergeleitet.</span><span class="sxs-lookup"><span data-stu-id="655d6-114">It's used by the Spark driver and will be passed to Spark executors.</span></span> <span data-ttu-id="655d6-115">Wenn diese Variable nicht eingerichtet ist, suchen die Spark-Executors den in der <code>PATH</code>-Umgebungsvariablen angegebenen Pfad.</span><span class="sxs-lookup"><span data-stu-id="655d6-115">If this variable is not set up, the Spark executors will search the path specified in the <code>PATH</code> environment variable.</span></span></br><span data-ttu-id="655d6-116">_z. B. "C:\bin\Microsoft.Spark.Worker"_</span><span class="sxs-lookup"><span data-stu-id="655d6-116">_e.g. "C:\bin\Microsoft.Spark.Worker"_</span></span>
| <span data-ttu-id="655d6-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span><span class="sxs-lookup"><span data-stu-id="655d6-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span></span> | <span data-ttu-id="655d6-118">Hierbei handelt es sich um durch Trennzeichen getrennte Pfade, in denen <code>Microsoft.Spark.Worker</code> Assemblys lädt.</span><span class="sxs-lookup"><span data-stu-id="655d6-118">Comma-separated paths where <code>Microsoft.Spark.Worker</code> will load assemblies.</span></span></br><span data-ttu-id="655d6-119">Beachten Sie, dass, wenn ein Pfad mit "." beginnt, das Arbeitsverzeichnis vorangestellt wird.</span><span class="sxs-lookup"><span data-stu-id="655d6-119">Note that if a path starts with ".", the working directory will be prepended.</span></span> <span data-ttu-id="655d6-120">Im **YARN-Modus** würde "." das Arbeitsverzeichnis des Containers darstellen.</span><span class="sxs-lookup"><span data-stu-id="655d6-120">If in **yarn mode**, "." would represent the container's working directory.</span></span></br><span data-ttu-id="655d6-121">_z. B. "C:\Users\\&lt;Benutzername&gt;\\&lt;mysparkapp&gt;\bin\Debug\\&lt;dotnet-Version&gt;"_</span><span class="sxs-lookup"><span data-stu-id="655d6-121">_e.g. "C:\Users\\&lt;user name&gt;\\&lt;mysparkapp&gt;\bin\Debug\\&lt;dotnet version&gt;"_</span></span>
| <span data-ttu-id="655d6-122">DOTNET_WORKER_DEBUG</span><span class="sxs-lookup"><span data-stu-id="655d6-122">DOTNET_WORKER_DEBUG</span></span>          | <span data-ttu-id="655d6-123">Wenn Sie <a href="https://github.com/dotnet/spark/blob/master/docs/developer-guide.md#debugging-user-defined-function-udf">ein UDF debuggen</a> wollen, legen Sie diese Umgebungsvariable auf <code>1</code> fest, bevor Sie <code>spark-submit</code> ausführen.</span><span class="sxs-lookup"><span data-stu-id="655d6-123">If you want to <a href="https://github.com/dotnet/spark/blob/master/docs/developer-guide.md#debugging-user-defined-function-udf">debug a UDF</a>, then set this environment variable to <code>1</code> before running <code>spark-submit</code>.</span></span>

### <a name="parameter-options"></a><span data-ttu-id="655d6-124">Parameteroptionen</span><span class="sxs-lookup"><span data-stu-id="655d6-124">Parameter options</span></span>
<span data-ttu-id="655d6-125">Sobald die Spark-Anwendung [gebündelt](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies) ist, können Sie diese mit `spark-submit` starten.</span><span class="sxs-lookup"><span data-stu-id="655d6-125">Once the Spark application is [bundled](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies), you can launch it using `spark-submit`.</span></span> <span data-ttu-id="655d6-126">In der folgenden Tabelle werden einige der häufig verwendeten Optionen aufgeführt:</span><span class="sxs-lookup"><span data-stu-id="655d6-126">The following table shows some of the commonly used options:</span></span>

| <span data-ttu-id="655d6-127">Parametername</span><span class="sxs-lookup"><span data-stu-id="655d6-127">Parameter Name</span></span>        | <span data-ttu-id="655d6-128">Beschreibung</span><span class="sxs-lookup"><span data-stu-id="655d6-128">Description</span></span>
| :---------------------| :----------
| <span data-ttu-id="655d6-129">--class</span><span class="sxs-lookup"><span data-stu-id="655d6-129">--class</span></span>               | <span data-ttu-id="655d6-130">Dies ist der Einstiegspunkt für Ihre Anwendung.</span><span class="sxs-lookup"><span data-stu-id="655d6-130">The entry point for your application.</span></span></br><span data-ttu-id="655d6-131">_z. B. org.apache.spark.deploy.dotnet.DotnetRunner_</span><span class="sxs-lookup"><span data-stu-id="655d6-131">_e.g. org.apache.spark.deploy.dotnet.DotnetRunner_</span></span>
| <span data-ttu-id="655d6-132">--master</span><span class="sxs-lookup"><span data-stu-id="655d6-132">--master</span></span>              | <span data-ttu-id="655d6-133">Dies ist die <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">master-URL</a> für den Cluster.</span><span class="sxs-lookup"><span data-stu-id="655d6-133">The <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">master URL</a> for the cluster.</span></span></br><span data-ttu-id="655d6-134">_z. B. YARN_</span><span class="sxs-lookup"><span data-stu-id="655d6-134">_e.g. yarn_</span></span>
| <span data-ttu-id="655d6-135">--deploy-mode</span><span class="sxs-lookup"><span data-stu-id="655d6-135">--deploy-mode</span></span>         | <span data-ttu-id="655d6-136">Dieser Parameter gibt an, ob Ihr Treiber auf den Workerknoten (<code>cluster</code>) oder lokal als externer Client (<code>client</code>) bereitgestellt werden soll.</span><span class="sxs-lookup"><span data-stu-id="655d6-136">Whether to deploy your driver on the worker nodes (<code>cluster</code>) or locally as an external client (<code>client</code>).</span></span></br><span data-ttu-id="655d6-137">Standardwert: <code>client</code></span><span class="sxs-lookup"><span data-stu-id="655d6-137">Default: <code>client</code></span></span>
| <span data-ttu-id="655d6-138">--conf</span><span class="sxs-lookup"><span data-stu-id="655d6-138">--conf</span></span>                | <span data-ttu-id="655d6-139">Dies ist eine beliebige Spark-Konfigurationseigenschaft im <code>key=value</code>-Format.</span><span class="sxs-lookup"><span data-stu-id="655d6-139">Arbitrary Spark configuration property in <code>key=value</code> format.</span></span></br><span data-ttu-id="655d6-140">_z. B. spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=.\worker\Microsoft.Spark.Worker_</span><span class="sxs-lookup"><span data-stu-id="655d6-140">_e.g. spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=.\worker\Microsoft.Spark.Worker_</span></span>
| <span data-ttu-id="655d6-141">--files</span><span class="sxs-lookup"><span data-stu-id="655d6-141">--files</span></span>               | <span data-ttu-id="655d6-142">Dies ist eine durch Trennzeichen getrennte Liste von Dateien, die im Arbeitsverzeichnis jedes Executors platziert werden sollen.</span><span class="sxs-lookup"><span data-stu-id="655d6-142">Comma-separated list of files to be placed in the working directory of each executor.</span></span><br/><ul><li><span data-ttu-id="655d6-143">Bitte beachten Sie, dass diese Option nur für den YARN-Modus anwendbar ist.</span><span class="sxs-lookup"><span data-stu-id="655d6-143">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="655d6-144">Diese unterstützt ähnlich wie bei Hadoop die Angabe von Dateinamen mit #.</span><span class="sxs-lookup"><span data-stu-id="655d6-144">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="655d6-145">_z. B. <code>myLocalSparkApp.dll#appSeen.dll</code>. Ihre Anwendung sollte den Namen als <code>appSeen.dll</code> verwenden, um auf <code>myLocalSparkApp.dll</code> zu verweisen, wenn sie unter YARN ausgeführt wird._</span><span class="sxs-lookup"><span data-stu-id="655d6-145">_e.g. <code>myLocalSparkApp.dll#appSeen.dll</code>. Your application should use the name as <code>appSeen.dll</code> to reference <code>myLocalSparkApp.dll</code> when running on YARN._</span></span></li>
| <span data-ttu-id="655d6-146">--archives</span><span class="sxs-lookup"><span data-stu-id="655d6-146">--archives</span></span>          | <span data-ttu-id="655d6-147">Dies ist eine durch Trennzeichen getrennte Liste von Archiven, die im Arbeitsverzeichnis jedes Executors extrahiert werden sollen.</span><span class="sxs-lookup"><span data-stu-id="655d6-147">Comma-separated list of archives to be extracted into the working directory of each executor.</span></span></br><ul><li><span data-ttu-id="655d6-148">Bitte beachten Sie, dass diese Option nur für den YARN-Modus anwendbar ist.</span><span class="sxs-lookup"><span data-stu-id="655d6-148">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="655d6-149">Diese unterstützt ähnlich wie bei Hadoop die Angabe von Dateinamen mit #.</span><span class="sxs-lookup"><span data-stu-id="655d6-149">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="655d6-150">_z. B. <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code>. Hiermit wird die ZIP-Datei in den <code>worker</code>-Ordner kopiert und extrahiert._</span><span class="sxs-lookup"><span data-stu-id="655d6-150">_e.g. <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code>. This will copy and extract the zip file to <code>worker</code> folder._</span></span></li>
| <span data-ttu-id="655d6-151">application-jar</span><span class="sxs-lookup"><span data-stu-id="655d6-151">application-jar</span></span>       | <span data-ttu-id="655d6-152">Dies ist der Pfad zu einer gebündelten JAR-Datei mit Ihrer Anwendung und allen Abhängigkeiten.</span><span class="sxs-lookup"><span data-stu-id="655d6-152">Path to a bundled jar including your application and all dependencies.</span></span></br><span data-ttu-id="655d6-153">_z. B. hdfs://&lt;Pfad-zu-Ihrer-JAR-Datei&gt;/microsoft-spark-&lt;Version&gt;.jar_</span><span class="sxs-lookup"><span data-stu-id="655d6-153">_e.g. hdfs://&lt;path to your jar&gt;/microsoft-spark-&lt;version&gt;.jar_</span></span>
| <span data-ttu-id="655d6-154">application-arguments</span><span class="sxs-lookup"><span data-stu-id="655d6-154">application-arguments</span></span> | <span data-ttu-id="655d6-155">Dies sind Argumente, die an die Hauptmethode Ihrer Hauptklasse übergeben werden, falls diese vorhanden sind.</span><span class="sxs-lookup"><span data-stu-id="655d6-155">Arguments passed to the main method of your main class, if any.</span></span></br><span data-ttu-id="655d6-156">_z. B. hdfs://&lt;Pfad-zu-Ihrer-App&gt;/&lt;Ihre-App&gt;.zip &lt;Ihr-App-Name&gt; &lt;App-Argumente&gt;_</span><span class="sxs-lookup"><span data-stu-id="655d6-156">_e.g. hdfs://&lt;path to your app&gt;/&lt;your app&gt;.zip &lt;your app name&gt; &lt;app args&gt;_</span></span>

> [!NOTE]
> <span data-ttu-id="655d6-157">Geben Sie beim Start von Anwendungen mit `spark-submit` alle `--options`-Parameter vor `application-jar` an, sonst werden sie ignoriert.</span><span class="sxs-lookup"><span data-stu-id="655d6-157">Specify all the `--options` before `application-jar` when launching applications with `spark-submit`, otherwise they will be ignored.</span></span> <span data-ttu-id="655d6-158">Weitere Informationen finden Sie unter [`spark-submit`-Optionen](https://spark.apache.org/docs/latest/submitting-applications.html) und [Details zur Ausführung von Spark unter YARN](https://spark.apache.org/docs/latest/running-on-yarn.html).</span><span class="sxs-lookup"><span data-stu-id="655d6-158">For more information, see [`spark-submit` options](https://spark.apache.org/docs/latest/submitting-applications.html) and [running spark on YARN details](https://spark.apache.org/docs/latest/running-on-yarn.html).</span></span>

## <a name="frequently-asked-questions"></a><span data-ttu-id="655d6-159">Häufig gestellte Fragen</span><span class="sxs-lookup"><span data-stu-id="655d6-159">Frequently asked questions</span></span>
### <a name="when-i-run-a-spark-app-with-udfs-i-get-a-filenotfoundexception-error-what-should-i-do"></a><span data-ttu-id="655d6-160">Wenn ich eine Spark-App mit UDFs ausführe, erhalte ich den Fehler \`FileNotFoundException'.</span><span class="sxs-lookup"><span data-stu-id="655d6-160">When I run a spark app with UDFs, I get a \`FileNotFoundException' error.</span></span> <span data-ttu-id="655d6-161">Wie sollte ich vorgehen?</span><span class="sxs-lookup"><span data-stu-id="655d6-161">What should I do?</span></span>
> <span data-ttu-id="655d6-162">**Fehler:** [Error] [TaskRunner] [0] ProcessStream() mit Ausnahme fehlgeschlagen: System.IO.FileNotFoundException: Assembly 'mySparkApp, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null' Datei nicht gefunden: 'mySparkApp.dll'</span><span class="sxs-lookup"><span data-stu-id="655d6-162">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.IO.FileNotFoundException: Assembly 'mySparkApp, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null' file not found: 'mySparkApp.dll'</span></span>

<span data-ttu-id="655d6-163">**Antwort:** Überprüfen Sie, ob die `DOTNET_ASSEMBLY_SEARCH_PATHS`-Umgebungsvariable korrekt festgelegt ist.</span><span class="sxs-lookup"><span data-stu-id="655d6-163">**Answer:** Check that the `DOTNET_ASSEMBLY_SEARCH_PATHS` environment variable is set correctly.</span></span> <span data-ttu-id="655d6-164">Diese sollte der Pfad sein, der `mySparkApp.dll` enthält.</span><span class="sxs-lookup"><span data-stu-id="655d6-164">It should be the path that contains your `mySparkApp.dll`.</span></span>

### <a name="after-i-upgraded-my-net-for-apache-spark-version-and-reset-the-dotnet_worker_dir-environment-variable-why-do-i-still-get-the-following-ioexception-error"></a><span data-ttu-id="655d6-165">Nachdem ich die .NET für Apache Spark-Version aktualisiert und die `DOTNET_WORKER_DIR`-Umgebungsvariable zurückgesetzt habe, erhalte ich immer noch die folgende `IOException`-Fehlermeldung. Warum?</span><span class="sxs-lookup"><span data-stu-id="655d6-165">After I upgraded my .NET for Apache Spark version and reset the `DOTNET_WORKER_DIR` environment variable, why do I still get the following `IOException` error?</span></span>
> <span data-ttu-id="655d6-166">**Fehler:** Lost task 0.0 in stage 11.0 (TID 24, Localhost, Executor Treiber): java.io.IOException: Programm kann nicht ausgeführt werden "Microsoft.Spark.Worker.exe": CreateProcess error=2, Das System kann die angegebene Datei nicht finden.</span><span class="sxs-lookup"><span data-stu-id="655d6-166">**Error:** Lost task 0.0 in stage 11.0 (TID 24, localhost, executor driver): java.io.IOException: Cannot run program "Microsoft.Spark.Worker.exe": CreateProcess error=2, The system cannot find the file specified.</span></span>

<span data-ttu-id="655d6-167">**Antwort:** Versuchen Sie zuerst, Ihr PowerShell-Fenster (oder andere Befehlsfenster) neu zu starten, damit es die neuesten Werte der Umgebungsvariablen annehmen kann.</span><span class="sxs-lookup"><span data-stu-id="655d6-167">**Answer:** Try restarting your PowerShell window (or other command windows) first so that it can take the latest environment variable values.</span></span> <span data-ttu-id="655d6-168">Starten Sie dann Ihr Programm.</span><span class="sxs-lookup"><span data-stu-id="655d6-168">Then start your program.</span></span>

### <a name="after-submitting-my-spark-application-i-get-the-error-systemtypeloadexception-could-not-load-type-systemruntimeremotingcontextscontext"></a><span data-ttu-id="655d6-169">Nachdem ich meine Spark-Anwendung übermittelt habe, erhalte ich die Fehlermeldung `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'`.</span><span class="sxs-lookup"><span data-stu-id="655d6-169">After submitting my Spark application, I get the error `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'`.</span></span>
> <span data-ttu-id="655d6-170">**Fehler:** [Error] [TaskRunner] [0] ProcessStream() mit Ausnahme fehlgeschlagen: System.TypeLoadException: Der Typ konnte nicht geladen werden 'System.Runtime.Remoting.Contexts.Context' from assembly 'mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=...'.</span><span class="sxs-lookup"><span data-stu-id="655d6-170">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context' from assembly 'mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=...'.</span></span>

<span data-ttu-id="655d6-171">**Antwort:** Überprüfen Sie die `Microsoft.Spark.Worker`-Version, die Sie verwenden.</span><span class="sxs-lookup"><span data-stu-id="655d6-171">**Answer:** Check the `Microsoft.Spark.Worker` version you are using.</span></span> <span data-ttu-id="655d6-172">Es gibt zwei Versionen: **.NET Framework 4.6.1** und **.NET Core 3.1.x**.</span><span class="sxs-lookup"><span data-stu-id="655d6-172">There are two versions: **.NET Framework 4.6.1** and **.NET Core 3.1.x**.</span></span> <span data-ttu-id="655d6-173">In diesem Fall sollte `Microsoft.Spark.Worker.net461.win-x64-<version>` (zum [Herunterladen](https://github.com/dotnet/spark/releases)) verwendet werden, da `System.Runtime.Remoting.Contexts.Context` nur für .NET Framework gilt.</span><span class="sxs-lookup"><span data-stu-id="655d6-173">In this case, `Microsoft.Spark.Worker.net461.win-x64-<version>` (which you can [download](https://github.com/dotnet/spark/releases)) should be used since `System.Runtime.Remoting.Contexts.Context` is only for .NET Framework.</span></span>

### <a name="how-do-i-run-my-spark-application-with-udfs-on-yarn-which-environment-variables-and-parameters-should-i-use"></a><span data-ttu-id="655d6-174">Wie führe ich meine Spark-Anwendung mit UDFs unter YARN aus?</span><span class="sxs-lookup"><span data-stu-id="655d6-174">How do I run my spark application with UDFs on YARN?</span></span> <span data-ttu-id="655d6-175">Welche Umgebungsvariablen und Parameter sollte ich verwenden?</span><span class="sxs-lookup"><span data-stu-id="655d6-175">Which environment variables and parameters should I use?</span></span>

<span data-ttu-id="655d6-176">**Antwort:** Die Umgebungsvariablen sollten als `spark.yarn.appMasterEnv.[EnvironmentVariableName]` angegeben werden, um die Spark-Anwendung auf YARN zu starten.</span><span class="sxs-lookup"><span data-stu-id="655d6-176">**Answer:** To launch the spark application on YARN, the environment variables should be specified as `spark.yarn.appMasterEnv.[EnvironmentVariableName]`.</span></span> <span data-ttu-id="655d6-177">Weiteres finden Sie unten im Beispiel, in dem `spark-submit` verwendet wird:</span><span class="sxs-lookup"><span data-stu-id="655d6-177">Please see below as an example using `spark-submit`:</span></span>

```powershell
spark-submit \
--class org.apache.spark.deploy.dotnet.DotnetRunner \
--master yarn \
--deploy-mode cluster \
--conf spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=./worker/Microsoft.Spark.Worker-<version> \
--conf spark.yarn.appMasterEnv.DOTNET_ASSEMBLY_SEARCH_PATHS=./udfs \
--archives hdfs://<path to your files>/Microsoft.Spark.Worker.net461.win-x64-<version>.zip#worker,hdfs://<path to your files>/mySparkApp.zip#udfs \
hdfs://<path to jar file>/microsoft-spark-<spark_majorversion-spark_minorversion>_<scala_majorversion.scala_minorversion>-<spark_dotnet_version>.jar \
hdfs://<path to your files>/mySparkApp.zip mySparkApp
```

## <a name="next-steps"></a><span data-ttu-id="655d6-178">Nächste Schritte</span><span class="sxs-lookup"><span data-stu-id="655d6-178">Next steps</span></span>

* [<span data-ttu-id="655d6-179">Erste Schritte mit .NET für Apache Spark</span><span class="sxs-lookup"><span data-stu-id="655d6-179">Get started with .NET for Apache Spark</span></span>](../tutorials/get-started.md)
* [<span data-ttu-id="655d6-180">Debuggen einer .NET für Apache Spark-Anwendung unter Windows</span><span class="sxs-lookup"><span data-stu-id="655d6-180">Debug a .NET for Apache Spark application on Windows</span></span>](debug.md)
