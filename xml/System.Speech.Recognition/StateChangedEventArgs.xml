<Type Name="StateChangedEventArgs" FullName="System.Speech.Recognition.StateChangedEventArgs">
  <TypeSignature Language="C#" Value="public class StateChangedEventArgs : EventArgs" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit StateChangedEventArgs extends System.EventArgs" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.StateChangedEventArgs" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.EventArgs</BaseTypeName>
  </Base>
  <Interfaces />
  <Docs>
    <summary>Gibt Daten von dem <see cref="E:System.Speech.Recognition.SpeechRecognizer.StateChanged" />-Ereignis zurück.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> Ereignis wird ausgelöst, durch die <xref:System.Speech.Recognition.SpeechRecognizer> Klasse. <xref:System.Speech.Recognition.StateChangedEventArgs>leitet sich von <xref:System.EventArgs> und an den Handler für übergeben <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> Ereignisse.  
  
 <xref:System.Speech.Recognition.SpeechRecognizer.State%2A>ist eine schreibgeschützte Eigenschaft. Eine freigegebene Spracherkennung Zustand kann nicht programmgesteuert geändert werden. Benutzer können eine freigegebene Spracherkennung Zustand mit der Spracherkennung-Benutzeroberfläche (UI) ändern oder über die **Spracherkennung** Mitglied der Windows **Systemsteuerung**.  
  
 Sowohl die **auf** und **Standbymodus** Einstellungen in der Sprache Recognition Benutzeroberfläche entsprechen den `Listening` Zustand. Die **deaktiviert** Einstellung in der Sprache Recognition Benutzeroberfläche entspricht <xref:System.Speech.Recognition.RecognizerState.Stopped>.  
  
   
  
## Examples  
 Im folgende Beispiel erstellt eine freigegebene Spracherkennung und erstellt dann auf zwei Arten von Grammatiken für die Erkennung von bestimmten Wörtern und kostenlose diktieren annimmt. Im Beispiel werden alle erstellten Grammatiken an die Erkennung asynchron geladen.  Einen Handler für das <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> Ereignis verwendet die <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A> Methode Windows Recognition in "Überwachungsmodus" aufgenommen werden sollen.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognizer recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize a shared speech recognition engine.  
      recognizer = new SpeechRecognizer();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted += new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized += new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Add a handler for the StateChanged event.  
      recognizer.StateChanged += new EventHandler<StateChangedEventArgs>(recognizer_StateChanged);  
  
      // Create "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yah}" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "nah" });  
      SemanticResultValue noValue = new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Put the shared speech recognizer into "listening" mode.  
    static void  recognizer_StateChanged(object sender, StateChangedEventArgs e)  
    {  
     if (e.RecognizerState != RecognizerState.Stopped)  
      {  
        recognizer.EmulateRecognizeAsync("Start listening");  
      }  
    }  
  
    // Write the grammar name and the text of the recognized phrase to the console.  
    static void  recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
     Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void  recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
     string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
      }  
  
      // Add exception handling code here.  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  }  
}  
  
```  
  
 ]]></format>
    </remarks>
  </Docs>
  <Members>
    <Member MemberName="RecognizerState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerState RecognizerState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.RecognizerState RecognizerState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.StateChangedEventArgs.RecognizerState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den aktuellen Zustand des freigegebenen Spracherkennungsmoduls in Windows ab.</summary>
        <value>Eine <see cref="T:System.Speech.Recognition.RecognizerState" />-Instanz, die angibt, ob der Zustand eines freigegebenen Spracherkennungsmoduls <see langword="Listening" /> oder <see langword="Stopped" /> ist.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Im folgenden Beispiel wird eine Anzeige der Zustandsinformationen gebotenen wird aktualisiert eine <xref:System.Speech.Recognition.RecognizerState> Instanz, die aus abgerufen der <xref:System.Speech.Recognition.StateChangedEventArgs.RecognizerState%2A> Eigenschaft eine <xref:System.Speech.Recognition.StateChangedEventArgs> Instanz übergeben, um einen Handler für ein <xref:System.Speech.Recognition.SpeechRecognizer.StateChanged> Ereignis.  
  
```  
  
// Make sure that _recognizer and recognition start buttons are disabled if state is stopped.  
// Re-enable the start button to allow manual re-enable if the speech recognizer is listening.  
_recognizer.StateChanged +=  
  delegate(object sender, StateChangedEventArgs eventArgs)   
{  
  _recognizerStateLabel.Text = "Speech Recognizer State: " + eventArgs.RecognizerState.ToString();  
};  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
  </Members>
</Type>
