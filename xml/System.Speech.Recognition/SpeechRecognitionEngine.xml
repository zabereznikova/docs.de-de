<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Bietet die Möglichkeit zum zugreifen und diese Verwalten einer in-Process-Spracherkennungsmoduls.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können eine Instanz dieser Klasse für keines der installierten Spracherkennung erstellen. Verwenden Sie zum Abrufen von Informationen über die Merkmale werden installiert die statische <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
 Diese Klasse ist für die Spracherkennung Recognition Module in-Process-Ausführung und ermöglicht die Steuerung über die verschiedenen Aspekte der Spracherkennung, wie folgt:  
  
-   Um eine in-Process-Spracherkennung zu erstellen, verwenden Sie eines der <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> Konstruktoren.  
  
-   Verwenden Sie zum Verwalten der Sprache Recognition Grammatiken der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> Methoden, und die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> Eigenschaft.  
  
-   Verwenden Sie zum Konfigurieren der Eingabe für die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> Methode.  
  
-   Um die Spracherkennung auszuführen, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methode.  
  
-   Verwenden Sie zum Bearbeiten der Behandlung der Erkennung auf Ruhe oder unerwarteter Eingaben die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
-   Um die Anzahl der Stellvertreter die Erkennung gibt zu ändern, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> Eigenschaft. Das Erkennungsmodul gibt Ergebnisse in einem <xref:System.Speech.Recognition.RecognitionResult> Objekt.  
  
-   Verwenden Sie zum Synchronisieren der Änderungen an die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode. Das Erkennungsmodul verwendet mehrere Threads, um Aufgaben auszuführen.  
  
-   Verwenden Sie zum Emulieren der Eingabe für die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> Objekt ist für die alleinige des Prozesses verwenden, die der Instanziierung des Objekts. Im Gegensatz dazu, die <xref:System.Speech.Recognition.SpeechRecognizer> basiert auf eine einzelne Erkennung wie jede Anwendung, die sie verwenden möchte.  
  
> [!NOTE]
>  Rufen Sie immer <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> , bevor Sie den letzten Verweis auf die von der Spracherkennung freigeben. Andernfalls werden die verwendeten Ressourcen werden nicht reserviert, bis der Garbage Collector des Erkennungsmodul-Objekts aufruft `Finalize` Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Da in diesem Beispiel verwendet die `Multiple` Modus der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> -Methode, er führt Recognition, bis Sie das Konsolenfenster schließen oder Beenden des Debuggens.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Initialisiert eine neue Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Klasse.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Für das Konstruieren einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> -Instanz anhand einer der folgenden:  
  
-   Die Standardeinstellung Spracherkennungsmoduls für das system  
  
-   Eine bestimmte Spracherkennungsmoduls an die von Ihnen anhand des Namens  
  
-   Die Standardeinstellung Spracherkennungsmoduls für ein Gebietsschema, die Sie angeben  
  
-   Eine bestimmte Erkennungsmodul, die die Kriterien, die Sie angeben erfüllen, in einem <xref:System.Speech.Recognition.RecognizerInfo> Objekt.  
  
 Bevor die von der Spracherkennung ausführen kann, müssen Sie mindestens eine Sprache Recognition Grammatik laden und konfigurieren die Eingabe für die Erkennung.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Initialisiert eine neue Instanz der dem <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> -Klasse unter Verwendung der Standardeinstellung von der Spracherkennung für das System.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Bevor die von der Spracherkennung Spracherkennung beginnen kann, müssen Sie mindestens eine Anerkennung Grammatik laden und konfigurieren die Eingabe für die Erkennung.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">Das Gebietsschema, das die Spracherkennung unterstützen muss.</param>
        <summary>Initialisiert eine neue Instanz der dem <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> -Klasse unter Verwendung der Standardeinstellung von der Spracherkennung für eine angegebene Gebietsschema.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Übernehmen alle gültigen Sprache / Land-Codes, Microsoft Windows und die System.Speech-API. Mit der Sprache, die im angegebenen Spracherkennung Ausführen der `CultureInfo` Argument, ein Spracherkennungsmodul, die Sprache / Land-Code installiert werden muss unterstützt. Die Spracherkennungsmoduls, die mit den im Lieferumfang von Microsoft Windows 7 arbeiten Sie mit der folgenden Sprache / Land-Codes.  
  
-   En-GB. Englisch (Großbritannien)  
  
-   En-US. Englisch (Vereinigte Staaten)  
  
-   de-DE. Deutsch (Deutschland)  
  
-   es-ES. Spanisch (Spanien)  
  
-   fr-FR Französisch (Frankreich)  
  
-   ja-JP. Japanisch (Japan)  
  
-   Zh-CN. Chinesisch (China)  
  
-   Zh-TW. Chinesisch (Taiwan)  
  
 Zwei Buchstaben bestehende Sprache codes, z. B. "En", "fr", oder ""es endenden"sind ebenfalls zulässig.  
  
 Bevor die von der Spracherkennung ausführen kann, müssen Sie mindestens eine Sprache Recognition Grammatik laden und konfigurieren die Eingabe für die Erkennung.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die demonstriert den grundlegenden Spracherkennung und initialisiert ein von der Spracherkennung für das Gebietsschema En-US.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Keines der installierten Spracherkennung unterstützen das angegebene Gebietsschema oder <paramref name="culture" /> die invariante Kultur.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Culture" /> ist <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">Die Informationen für die bestimmten von der Spracherkennung.</param>
        <summary>Initialisiert eine neue Instanz der dem <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> anhand der Informationen in einer <see cref="T:System.Speech.Recognition.RecognizerInfo" /> Objekt an die Erkennung zu verwenden.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können eine Instanz dieser Klasse für keines der installierten Spracherkennung erstellen. Verwenden Sie zum Abrufen von Informationen darüber, welche Merkmale werden installiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
 Bevor die von der Spracherkennung ausführen kann, müssen Sie mindestens eine Sprache Recognition Grammatik laden und konfigurieren die Eingabe für die Erkennung.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die demonstriert den grundlegenden Spracherkennung und initialisiert ein von der Spracherkennung die englische Sprache unterstützt.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">Der token Name, der die von der Spracherkennung verwendet werden soll.</param>
        <summary>Initialisiert eine neue Instanz der dem <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Klasse mit einem Zeichenfolgenparameter, der angibt, den Namen der Erkennung zu verwenden.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die token-Namen der Erkennung ist der Wert des der <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizerInfo> zurückgegebenes Objekt die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> Eigenschaft der Erkennung. Um eine Auflistung aller installierten Prüfer abzurufen, verwenden Sie die statische <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
 Bevor die von der Spracherkennung ausführen kann, müssen Sie mindestens eine Sprache Recognition Grammatik laden und konfigurieren die Eingabe für die Erkennung.  
  
 Rufen Sie zum Laden einer Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht, und erstellt eine Instanz von der Spracherkennung Erkennungsmodul 8.0 für Windows (Englisch - USA).  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Keine von der Spracherkennung mit diesem token Namen installiert ist, oder <paramref name="recognizerId" /> ist eine leere Zeichenfolge ("").</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="recognizerId" /> ist <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Format der das Audio empfangen werden, indem Sie die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Das Format der Audio an die Eingabe für die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Instanz oder <see langword="null" /> , wenn die Eingabe ist nicht konfiguriert oder auf die null-Eingabe festgelegt.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Um die Audioeingabe zu konfigurieren, verwenden Sie eine der folgenden Methoden:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 Im folgenden Beispiel wird <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> abzurufen und Audioformat Daten anzuzeigen.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft das Maß an das Audio empfangen werden, indem Sie die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Das audio Maß die Eingabe für die Spracherkennung zwischen 0 und 100 an.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Der Wert 0 stellt Ruhe und 100 steht für die maximale Eingabe Volume.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> meldet das Maß an seine Audioeingabe.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis mehrere Male pro Sekunde. Die Häufigkeit, mit der das Ereignis ausgelöst wird, hängt von dem Computer, auf dem die Anwendung ausgeführt wird.  
  
 Verwenden Sie zum Abrufen der audio Ebene zum Zeitpunkt des Ereignisses die <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Um die aktuelle audio Ebene der Eingabe für die Erkennung zu erhalten, verwenden Sie der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> Eigenschaft.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel fügt einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> Ereignis, um eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> Objekt. Der Handler gibt die neue audio Ebene in der Konsole aus.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft die aktuelle Position in den Audiostream generiert wird, von dem Gerät, das Bereitstellen von Eingaben für die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Die aktuelle Position in den Audiostream, der durch das Eingabegerät, das generiert wird.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> -Eigenschaft verweist auf das Eingabegerät Position in der generierten Audiostream. Im Gegensatz dazu, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Eigenschaft verweist auf die Erkennung Position innerhalb der Audioeingabe. Diese Positionen können unterschiedlich sein. Z. B. wenn die Erkennung erhalten hat Eingabe nicht für die It hat noch erzeugt ein Erkennungsergebnis wird der Wert der die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> -Eigenschaft muss kleiner als der Wert von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Eigenschaft.  
  
   
  
## Examples  
 Im folgenden Beispiel verwendet die in-Process-Spracherkennung diktieren Grammatik Spracheingabe entsprechend an. Einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignis in die Konsole schreibt die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> Wenn erkennt die von der Spracherkennung Sprache bei der Eingabe.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> das Audiosignal ein Problem festgestellt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Abrufen, welches Problem ist aufgetreten, der <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel definiert einen Ereignishandler, die sammelt Informationen über ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> Ereignis.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den Status der Audiodatei empfangen werden, indem Sie die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Der Status der audio Eingabe, die von der Spracherkennung.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> Eigenschaft darstellt, das audio Zustand mit einem Mitglied der <xref:System.Speech.Recognition.AudioState> Enumeration.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn die statusänderungen im Audioelement empfängt die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Abrufen des audio Zustands zum Zeitpunkt des Ereignisses die <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Um den aktuellen audio-Status, der die Eingabe für die Erkennung zu erhalten, verwenden Sie der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> Eigenschaft. Weitere Informationen zum audio-Status finden Sie unter der <xref:System.Speech.Recognition.AudioState> Enumeration.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgenden Beispiel wird einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> Ereignis, um die Erkennung Schreiben des neuen <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> an die Konsole jedes Mal, wenn sie Änderungen, die mithilfe eines Members der <xref:System.Speech.Recognition.AudioState> Enumeration.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft ab oder legt das Zeitintervall fest, währenddessen ein <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Eingabe enthaltenden nur Hintergrundgeräuschen, vor dem Abschließen der Erkennung akzeptiert.</summary>
        <value>Die Dauer des Zeitintervalls.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede von der Spracherkennung verfügt über einen Algorithmus, um die Unterscheidung zwischen Ruhe und Sprache. Das Erkennungsmodul klassifiziert Hintergrundgeräuschen alle Ruhe-Eingabedatei, die die erste Regel aller der Erkennung nicht übereinstimmen geladen und der Sprache Recognition Grammatiken aktiviert. Wenn die Erkennung nur Hintergrundgeräuschen und Ruhe innerhalb des Timeoutintervalls Babble empfängt, schließt die Erkennung dieser Erkennungsvorgang ab.  
  
-   Für asynchrone Erkennungsvorgänge, löst die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> -Ereignis, in dem die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> Eigenschaft ist `true`, und die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> Eigenschaft ist `null`.  
  
-   Synchrone Erkennungsvorgänge Emulation und die Erkennung gibt `null`, anstatt eine gültige <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Wenn das Timeout für die Babble auf 0 festgelegt ist, führt die Erkennung keine Babble Timeout-Überprüfung aus. Das Timeoutintervall kann es sich um einen nicht negativen Wert annehmen. Der Standardwert beträgt 0 Sekunden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht, der festlegt der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> vor dem Initiieren der Spracherkennung. Handler für die Spracherkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignisse Ausgabe Ereignisinformationen in die Konsole zur Veranschaulichung der Funktion wie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> auf Recognition Vorgänge auswirken.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft ist kleiner als 0 Sekunden festgelegt.</exception>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Verwirft die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Objekt.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">
          <see langword="true" />, um sowohl verwaltete als auch nicht verwaltete Ressourcen freizugeben, <see langword="false" />, um ausschließlich nicht verwaltete Ressourcen freizugeben.</param>
        <summary>Verwirft das <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />-Objekt und gibt Ressourcen frei, die während der Sitzung verwendet werden.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuliert Eingabe für die Spracherkennung Text anstelle von Audio für die synchrone Spracherkennung verwenden.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden der Audioeingabe System zu umgehen, und geben Sie Text an, um die Erkennung als <xref:System.String> Objekte oder als ein Array von <xref:System.Speech.Recognition.RecognizedWordUnit> Objekte. Dies kann hilfreich sein, wenn Sie testen oder Debuggen einer Anwendung oder Grammatik. Sie können z. B. Emulation verwenden, um zu bestimmen, ob ein Wort in einem Grammatik ist und welche Semantik zurückgegeben werden, wenn das Wort erkannt wird. Verwenden der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> Methode Audioeingabe an die Spracherkennungsmoduls während Emulation Vorgänge deaktiviert.  
  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Das Erkennungsmodul neue Zeilen und zusätzlichen Leerraum ignoriert und Interpunktion als literal Eingabe behandelt.  
  
> [!NOTE]
>  Die <xref:System.Speech.Recognition.RecognitionResult> Objekt, die von der Spracherkennung generiert wurde als Antwort auf "emuliert" Eingabe hat den Wert der `null` für seine <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> Eigenschaft.  
  
 Zum Emulieren der asynchronen Recognition verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methode.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe für den Erkennungsvorgang.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung Text anstelle von Audio für die synchrone Spracherkennung verwenden.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" /> , wenn der Vorgang nicht erfolgreich ist oder die Erkennung nicht aktiviert.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird.  
  
 Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren und Breite Zeichen, beim Anwenden der Grammatikregeln für den auf der input-Ausdruck. Weitere Informationen über diese Art von Vergleich finden Sie unter der <xref:System.Globalization.CompareOptions> Enumerationswerte <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> und <xref:System.Globalization.CompareOptions.IgnoreWidth>. Der Prüfer ist außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Interpunktion als literal Eingabe behandelt.  
  
   
  
## Examples  
 Im folgenden Codebeispiel ist Teil einer Konsolenanwendung, die "emuliert" Eingabe, die zugehörigen Erkennungsergebnisse und die zugehörigen Ereignisse ausgelöst, die für die von der Spracherkennung veranschaulicht. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Das Erkennungsmodul verfügt über keine Spracherkennung Recognition Grammatiken geladen.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Ein Array von Word-Einheiten, die die Eingabe für den Erkennungsvorgang enthält.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die beschreiben, den Typ des Vergleichs, der für den emulierten Erkennungsvorgang verwendet werden soll.</param>
        <summary>Eingabe von bestimmten Wörtern für die Spracherkennung mit Text anstelle von Audio für die synchrone Spracherkennung emuliert und gibt an, wie die Erkennung für Unicode-Vergleich zwischen den Wörtern und die geladenen Speech Recognition Grammatiken behandelt.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" /> , wenn der Vorgang nicht erfolgreich ist oder die Erkennung nicht aktiviert.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird.  
  
 Das Erkennungsmodul verwendet `compareOptions` Wenn der input-Ausdruck als Grammatikregeln anwendet. Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Das Erkennungsmodul immer die Zeichenbreite ignoriert und nie vom Typ Kana ignoriert. Das Erkennungsmodul auch neue Zeilen und zusätzliche Leerzeichen werden ignoriert und Interpunktion als literal Eingabe behandelt. Weitere Informationen zu der Zeichenbreite und Kanatyp, finden Sie unter der <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Das Erkennungsmodul verfügt über keine Spracherkennung Recognition Grammatiken geladen.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" />enthält eine oder mehrere <see langword="null" /> Elemente.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" />enthält die <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, oder <see cref="F:System.Globalization.CompareOptions.StringSort" /> Flag.</exception>
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe-Ausdruck für den Erkennungsvorgang.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die beschreiben, den Typ des Vergleichs, der für den emulierten Erkennungsvorgang verwendet werden soll.</param>
        <summary>Eingabe eines Ausdrucks für die Spracherkennung mit Text anstelle von Audio für die synchrone Spracherkennung emuliert, und gibt an, wie die Erkennung für Unicode-Vergleich zwischen den Ausdruck und die geladenen Speech Recognition Grammatiken behandelt.</summary>
        <returns>Das Ergebnis für den Erkennungsvorgang oder <see langword="null" /> , wenn der Vorgang nicht erfolgreich ist oder die Erkennung nicht aktiviert.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird.  
  
 Das Erkennungsmodul verwendet `compareOptions` Wenn der input-Ausdruck als Grammatikregeln anwendet. Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Das Erkennungsmodul immer die Zeichenbreite ignoriert und nie vom Typ Kana ignoriert. Das Erkennungsmodul auch neue Zeilen und zusätzliche Leerzeichen werden ignoriert und Interpunktion als literal Eingabe behandelt. Weitere Informationen zu der Zeichenbreite und Kanatyp, finden Sie unter der <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Das Erkennungsmodul verfügt über keine Spracherkennung Recognition Grammatiken geladen.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" />enthält die <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, oder <see cref="F:System.Globalization.CompareOptions.StringSort" /> Flag.</exception>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emuliert Eingabe für die Spracherkennung Text anstelle von Audio für die asynchrone Spracherkennung verwenden.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden der Audioeingabe System zu umgehen, und geben Sie Text an, um die Erkennung als <xref:System.String> Objekte oder als ein Array von <xref:System.Speech.Recognition.RecognizedWordUnit> Objekte. Dies kann hilfreich sein, wenn Sie testen oder Debuggen einer Anwendung oder Grammatik. Sie können z. B. Emulation verwenden, um zu bestimmen, ob ein Wort in einem Grammatik ist und welche Semantik zurückgegeben werden, wenn das Wort erkannt wird. Verwenden der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> Methode Audioeingabe an die Spracherkennungsmoduls während Emulation Vorgänge deaktiviert.  
  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Recognition-Vorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis. Das Erkennungsmodul neue Zeilen und zusätzlichen Leerraum ignoriert und Interpunktion als literal Eingabe behandelt.  
  
> [!NOTE]
>  Die <xref:System.Speech.Recognition.RecognitionResult> Objekt, die von der Spracherkennung generiert wurde als Antwort auf "emuliert" Eingabe hat den Wert der `null` für seine <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> Eigenschaft.  
  
 Zum emulieren synchrone Recognition verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> Methode.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe für den Erkennungsvorgang.</param>
        <summary>Emuliert die Eingabe eines Ausdrucks für die Spracherkennung Text anstelle von Audio für die asynchrone Spracherkennung verwenden.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Recognition-Vorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
 Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren und Breite Zeichen, beim Anwenden der Grammatikregeln für den auf der input-Ausdruck. Weitere Informationen über diese Art von Vergleich finden Sie unter der <xref:System.Globalization.CompareOptions> Enumerationswerte <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> und <xref:System.Globalization.CompareOptions.IgnoreWidth>. Der Prüfer ist außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Interpunktion als literal Eingabe behandelt.  
  
   
  
## Examples  
 Im folgenden Codebeispiel ist Teil einer Konsolenanwendung, die asynchrone emulierten Eingabe, die zugeordneten Erkennungsergebnisse und die zugehörigen Ereignisse ausgelöst, die für die von der Spracherkennung veranschaulicht. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung ist keine Sprache Recognition Grammatiken geladen, oder die Erkennung hat eine asynchrone Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Ein Array von Word-Einheiten, die die Eingabe für den Erkennungsvorgang enthält.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die beschreiben, den Typ des Vergleichs, der für den emulierten Erkennungsvorgang verwendet werden soll.</param>
        <summary>Bestimmte Wörter, die von der Spracherkennung, mit der ein Array von Eingabe emuliert <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> anstelle von Audio für die asynchrone Spracherkennung-Objekte und gibt an, wie die Erkennung für Unicode-Vergleich zwischen den Wörtern und die geladenen Spracherkennung behandelt Erkennung Grammatiken.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Recognition-Vorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
 Das Erkennungsmodul verwendet `compareOptions` Wenn der input-Ausdruck als Grammatikregeln anwendet. Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Der Prüfer immer die Zeichenbreite ignoriert und nie den Kanatyp ignorieren. Der Prüfer ist außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Interpunktion als literal Eingabe behandelt. Weitere Informationen zu der Zeichenbreite und Kanatyp, finden Sie unter der <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung ist keine Sprache Recognition Grammatiken geladen, oder die Erkennung hat eine asynchrone Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" />enthält eine oder mehrere <see langword="null" /> Elemente.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" />enthält die <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, oder <see cref="F:System.Globalization.CompareOptions.StringSort" /> Flag.</exception>
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Die Eingabe-Ausdruck für den Erkennungsvorgang.</param>
        <param name="compareOptions">Eine bitweise Kombination der Enumerationswerte, die beschreiben, den Typ des Vergleichs, der für den emulierten Erkennungsvorgang verwendet werden soll.</param>
        <summary>Eingabe eines Ausdrucks für die von der Spracherkennung, mithilfe von Text anstelle von Audio für die asynchrone Spracherkennung emuliert, und gibt an, wie die Erkennung für Unicode-Vergleich zwischen den Ausdruck und die geladenen Speech Recognition Grammatiken behandelt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die Spracherkennung Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse wie bei der Erkennungsvorgang nicht emuliert wird. Wenn die Erkennung der asynchronen Recognition-Vorgang abgeschlossen ist, löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
 Das Erkennungsmodul verwendet `compareOptions` Wenn der input-Ausdruck als Grammatikregeln anwendet. Die Merkmale, die mit Vista und Windows 7 ausgeliefert Groß-/Kleinschreibung ignorieren, wenn die <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> oder <xref:System.Globalization.CompareOptions.IgnoreCase> Wert vorhanden ist. Der Prüfer immer die Zeichenbreite ignoriert und nie den Kanatyp ignorieren. Der Prüfer ist außerdem neue Zeilen und zusätzlichen Leerraum zu ignorieren und Interpunktion als literal Eingabe behandelt. Weitere Informationen zu der Zeichenbreite und Kanatyp, finden Sie unter der <xref:System.Globalization.CompareOptions> Enumeration.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Die Erkennung ist keine Sprache Recognition Grammatiken geladen, oder die Erkennung hat eine asynchrone Erkennungsvorgang, der noch nicht abgeschlossen ist.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" />enthält die <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, oder <see cref="F:System.Globalization.CompareOptions.StringSort" /> Flag.</exception>
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> schließt einen asynchronen Erkennungsvorgang emulierten Eingabe ab.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methode startet einen asynchronen Recognition-Vorgang. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis aus, wenn sie den asynchronen Vorgang abschließt.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Vorgang auslösen kann die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis ist die letzte einem solchen Fall, dass die Erkennung für einen angegebenen Vorgang löst.  
  
 Wenn "emuliert" Erkennung erfolgreich war, können Sie die Erkennungsergebnis mithilfe einer der folgenden zugreifen:  
  
-   Die <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> Eigenschaft in der <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> Objekt im Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis.  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>-Eigenschaft in der <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> Objekt im Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis.  
  
 Wenn "emuliert" Erkennung nicht erfolgreich war, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis wird nicht ausgelöst, und die <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> wird null sein.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> wird von <xref:System.ComponentModel.AsyncCompletedEventArgs> abgeleitet.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> wird von <xref:System.Speech.Recognition.RecognitionEventArgs> abgeleitet.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>-Delegaten geben Sie die Methode für die Ereignisbehandlung an. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung, die lädt eine Spracherkennung Recognition Grammatik und asynchrone emulierten Eingabe, die zugeordneten Erkennungsergebnisse und die zugehörigen Ereignisse ausgelöst, die für die von der Spracherkennung veranschaulicht.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft ab oder legt dem Intervall der Pause, die die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> vor dem Abschließen eines Erkennungsvorgangs an das Ende eindeutig Eingabe akzeptiert.</summary>
        <value>Die Dauer des Intervalls Stille.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die von der Spracherkennung verwendet dieses Timeoutintervall an, wenn die Eingabe Recognition eindeutig ist. Z. B. für eine Sprache Recognition-Grammatik, mit der Erkennung von beidem unterstützt "neue Bitte game" oder "new Game", "neue Bitte game" ist eine eindeutige Eingabe, und "new Game" ist eine mehrdeutige Eingabe.  
  
 Diese Eigenschaft bestimmt, wie lange die Spracherkennungsmoduls für zusätzliche Eingabe-gewartet wird, vor dem Abschließen eines Erkennungsvorgangs. Das Timeout-Intervall kann von 0 Sekunden auf 10 Sekunden, einschließlich sein. Der Standardwert beträgt 150 Millisekunden.  
  
 Um das Timeoutintervall für mehrdeutigen Eingaben festzulegen, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft ist kleiner als 0 Sekunden oder größer als 10 Sekunden festgelegt.</exception>
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft ab oder legt dem Intervall der Pause, die die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> vor dem Abschließen eines Erkennungsvorgangs am Ende der mehrdeutigen Eingaben akzeptiert.</summary>
        <value>Die Dauer des Intervalls Stille.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die von der Spracherkennung verwendet dieses Timeoutintervall an, wenn die Eingabe Recognition mehrdeutig ist. Z. B. für eine Sprache Recognition-Grammatik, mit der Erkennung von beidem unterstützt "neue Bitte game" oder "new Game", "neue Bitte game" ist eine eindeutige Eingabe, und "new Game" ist eine mehrdeutige Eingabe.  
  
 Diese Eigenschaft bestimmt, wie lange die Spracherkennungsmoduls für zusätzliche Eingabe-gewartet wird, vor dem Abschließen eines Erkennungsvorgangs. Das Timeout-Intervall kann von 0 Sekunden auf 10 Sekunden, einschließlich sein. Der Standardwert beträgt 500 Millisekunden.  
  
 Verwenden Sie zum Festlegen der Timeout-Intervall für die Eingabe eindeutig die <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaft.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft ist kleiner als 0 Sekunden oder größer als 10 Sekunden festgelegt.</exception>
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft eine Auflistung von der <see cref="T:System.Speech.Recognition.Grammar" /> Objekte, die in diesem geladen werden <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Instanz.</summary>
        <value>Die Auflistung von <see cref="T:System.Speech.Recognition.Grammar" />-Objekten</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Das folgende Beispiel gibt Informationen an die Konsole für jede Sprache Recognition-Grammatik, die derzeit von einer Spracherkennung geladen wird.  
  
> [!IMPORTANT]
>  Kopieren Sie die Grammatik-Auflistung, um Fehler zu vermeiden, wenn die Auflistung geändert wird, während diese Methode listet die Elemente der Auflistung auf.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft ab oder legt das Zeitintervall fest, währenddessen ein <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Eingabe enthaltenden nur Ruhe vor dem Abschließen der Erkennung akzeptiert.</summary>
        <value>Die Dauer des Intervalls Stille.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede von der Spracherkennung verfügt über einen Algorithmus, um die Unterscheidung zwischen Ruhe und Sprache. Wenn die Eingabe für die Erkennung Ruhe innerhalb des Timeoutzeitraums anfängliche Ruhe ist, schließt die Erkennung dieser Erkennungsvorgang ab.  
  
-   Für asynchrone Erkennungsvorgänge und Emulation, löst die Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> -Ereignis, in dem die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> Eigenschaft ist `true`, und die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> Eigenschaft ist `null`.  
  
-   Synchrone Erkennungsvorgänge Emulation und die Erkennung gibt `null`, anstatt eine gültige <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Wenn das erste Ruhe-Timeout-Intervall auf 0 festgelegt ist, führt die Erkennung keine anfängliche Ruhe-Timeout-Überprüfung aus. Das Timeoutintervall kann es sich um einen nicht negativen Wert annehmen. Der Standardwert beträgt 0 Sekunden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> vor dem Initiieren der Spracherkennung. Handler für die Spracherkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignisse Ausgabe Ereignisinformationen in die Konsole zu veranschaulichen, wie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften des eine <xref:System.Speech.Recognition.SpeechRecognitionEngine> Eigenschaften wirken sich auf Erkennungsvorgänge.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Diese Eigenschaft ist kleiner als 0 Sekunden festgelegt.</exception>
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Gibt Informationen für alle von der installierten Spracherkennung des aktuellen Systems zurück.</summary>
        <returns>Eine schreibgeschützte Auflistung von der <see cref="T:System.Speech.Recognition.RecognizerInfo" /> Objekte, die die installierte Merkmale zu beschreiben.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Abrufen von Informationen über die aktuellen Erkennung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> Eigenschaft.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird die zurückgegebene Auflistung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode, um eine von der Spracherkennung zu ermitteln, die die englische Sprache unterstützt.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Das grammatikobjekt geladen.</param>
        <summary>Synchron lädt ein <see cref="T:System.Speech.Recognition.Grammar" /> Objekt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Das Erkennungsmodul löst eine Ausnahme aus, wenn die <xref:System.Speech.Recognition.Grammar> Objekt bereits geladen wird, wird asynchron geladen oder konnte nicht in jeder Erkennungsmodul geladen. Kann nicht geladen werden, die gleiche <xref:System.Speech.Recognition.Grammar> Objekt in mehreren Instanzen von <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Erstellen Sie stattdessen ein neues <xref:System.Speech.Recognition.Grammar> -Objekt für jedes <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz.  
  
 Wenn die Erkennung ausgeführt wird, müssen Anwendungen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> der Spracherkennungsmoduls vor dem Laden, entladen, aktivieren oder deaktivieren eine Grammatik anhalten.  
  
 Wenn Sie eine Grammatik laden, ist es standardmäßig aktiviert. Verwenden Sie zum Deaktivieren einer geladenen Grammatik der <xref:System.Speech.Recognition.Grammar.Enabled%2A> Eigenschaft.  
  
 Beim Laden einer <xref:System.Speech.Recognition.Grammar> -Objekt asynchron, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar> und lädt sie in einer von der Spracherkennung.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" />ist nicht in einem gültigen Zustand.</exception>
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Die Spracherkennung Recognition Grammatik geladen.</param>
        <summary>Lädt asynchron eine Spracherkennung Recognition-Grammatik.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Beim Abschluss der Erkennung Laden einer <xref:System.Speech.Recognition.Grammar> -Objekt, löst es eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> Ereignis. Das Erkennungsmodul löst eine Ausnahme aus, wenn die <xref:System.Speech.Recognition.Grammar> Objekt bereits geladen wird, wird asynchron geladen oder konnte nicht in jeder Erkennungsmodul geladen. Kann nicht geladen werden, die gleiche <xref:System.Speech.Recognition.Grammar> Objekt in mehreren Instanzen von <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Erstellen Sie stattdessen ein neues <xref:System.Speech.Recognition.Grammar> -Objekt für jedes <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz.  
  
 Wenn die Erkennung ausgeführt wird, müssen Anwendungen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> der Spracherkennungsmoduls vor dem Laden, entladen, aktivieren oder deaktivieren eine Grammatik anhalten.  
  
 Wenn Sie eine Grammatik laden, ist es standardmäßig aktiviert. Verwenden Sie zum Deaktivieren einer geladenen Grammatik der <xref:System.Speech.Recognition.Grammar.Enabled%2A> Eigenschaft.  
  
 Um eine Sprache Recognition Grammatik synchron zu laden, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> Methode.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" />ist nicht in einem gültigen Zustand.</exception>
        <exception cref="T:System.OperationCanceledException">Der asynchrone Vorgang wurde abgebrochen.</exception>
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> abgeschlossen ist das asynchrone Laden des ein <see cref="T:System.Speech.Recognition.Grammar" /> Objekt.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Des Erkennungsmodul <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode initiiert einen asynchronen Vorgang. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis, wenn sie den Vorgang abgeschlossen ist. Zum Abrufen der <xref:System.Speech.Recognition.Grammar> Objekt, das die Erkennung laden, verwenden Sie die <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Abrufen des aktuellen <xref:System.Speech.Recognition.Grammar> Objekte, die die Erkennung geladen wurde, verwenden Sie der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> Eigenschaft.  
  
 Wenn die Erkennung ausgeführt wird, müssen Anwendungen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> der Spracherkennungsmoduls vor dem Laden, entladen, aktivieren oder deaktivieren eine Grammatik anhalten.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel erstellt ein in-Process-Spracherkennung und erstellt dann auf zwei Arten von Grammatiken für die Erkennung von bestimmten Wörtern und kostenlose diktieren annimmt. Im Beispiel wird erstellt eine <xref:System.Speech.Recognition.Grammar> Objekt aus allen Grammatiken Recognition abgeschlossenen Spracherkennung asynchron lädt dann die <xref:System.Speech.Recognition.Grammar> -Objekte und die <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz. Handler für der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Schreiben von Ereignissen in der Konsole den Namen des dem <xref:System.Speech.Recognition.Grammar> -Objekt, das verwendet wurde, um die Erkennung und den Text des Ergebnisses Recognition bzw. ausführen.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft ab oder legt der maximalen Anzahl von alternativen Erkennungsergebnisse, die die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> gibt für jede Erkennungsvorgang zurück.</summary>
        <value>Die Anzahl der zurückzugebenden alternative Ergebnisse.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognitionResult> Klasse enthält die Auflistung der <xref:System.Speech.Recognition.RecognizedPhrase> Objekte, die mögliche Interpretationen der Eingabe darstellen.  
  
 Der Standardwert für <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> ist 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">
          <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />wird auf einen Wert kleiner als 0 festgelegt.</exception>
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der zurückzugebenden Einstellung.</param>
        <summary>Gibt die Werte der Einstellungen für die Erkennung zurück.</summary>
        <returns>Der Wert der Einstellung.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Erkennung Einstellungen können Zeichenfolge, 64-Bit-Ganzzahl oder Arbeitsspeicher-Adressdaten enthalten. Die folgende Tabelle beschreibt die Einstellungen, die für eine Microsoft-Speech-API (SAPI) definiert sind – Erkennung kompatibel. Die folgenden Einstellungen benötigen denselben Bereich für jeden Erkennungsmodul, die die Einstellung unterstützt. Eine SAPI-kompatible Erkennungsmodul ist nicht erforderlich, diese Einstellungen unterstützt und kann andere Einstellungen unterstützen.  
  
|Name|Beschreibung|  
|----------|-----------------|  
|`ResourceUsage`|Gibt die Erkennung CPU-Auslastung an. Der Bereich liegt zwischen 0 und 100. Der Standardwert ist 50.|  
|`ResponseSpeed`|Gibt die Länge der Pause am Ende der Eingabe eindeutig an, bevor die von der Spracherkennung ein Erkennungsvorgangs abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 Millisekunden (ms). Diese Einstellung bezieht sich auf der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaft.  Standard = 150ms.|  
|`ComplexResponseSpeed`|Gibt die Länge der Pause am Ende der mehrdeutigen Eingaben an, bevor die von der Spracherkennung ein Erkennungsvorgangs abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 ms. Diese Einstellung bezieht sich auf der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft. Standard = 500 ms.|  
|`AdaptationOn`|Gibt an, ob die Anpassung des Modells acoustic auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`). Der Standardwert ist `1` (ON).|  
|`PersistedBackgroundAdaptation`|Gibt an, ob die Anpassung im Hintergrund auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`), und speichert die Einstellung in der Registrierung. Der Standardwert ist `1` (ON).|  
  
 Um eine Einstellung für die Erkennung zu aktualisieren, verwenden Sie eines der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden.  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung, die die Werte für eine Reihe von den Einstellungen für die Erkennung, die das Gebietsschema En-US unterstützt definierten ausgibt. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Das Erkennungsmodul verfügt nicht über für eine Einstellung mit diesem Namen.</exception>
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Startet einen synchronen Speech Recognition-Vorgang.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden ausführen ein einzelnen, synchrone Erkennungsvorgangs an. Das Erkennungsmodul führt diesen Vorgang für seine geladen und aktiviert Speech Recognition Grammatiken.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
 Das Erkennungsmodul wird nicht ausgelöst. die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis bei Verwendung einer der der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> -Methoden zurückgeben einer <xref:System.Speech.Recognition.RecognitionResult> -Objekt, oder `null` , wenn der Vorgang nicht erfolgreich ist oder die Erkennung nicht aktiviert.  
  
 Eine synchrone Erkennungsvorgang kann aus den folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften oder für die `initialSilenceTimeout` Parameter von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methode.  
  
-   Das Erkennungsmodul Spracherkennung erkennt jedoch findet keine Übereinstimmungen in einer seiner geladen und aktiviert <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Verwenden Sie zum Ändern, wie die Erkennung für die zeitliche Abfolge der Sprache oder Pausen in Bezug auf Erkennung behandelt, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> benötigen mindestens ein <xref:System.Speech.Recognition.Grammar> Objekt vor dem Ausführen von Recognition geladen. Um eine Sprache Recognition Grammatik zu laden, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
 Um asynchrone Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Führt einen synchronen Speech Recognition aus.</summary>
        <returns>Das Erkennungsergebnis für die Eingabe oder <see langword="null" /> , wenn der Vorgang nicht erfolgreich ist oder die Erkennung nicht aktiviert.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode führt eine einzelne Erkennungsvorgang. Das Erkennungsmodul führt diesen Vorgang für seine geladen und aktiviert Speech Recognition Grammatiken.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
 Das Erkennungsmodul wird nicht ausgelöst. die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis bei Verwendung dieser Methode.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> Methode gibt ein <xref:System.Speech.Recognition.RecognitionResult> -Objekt, oder `null` , wenn der Vorgang nicht erfolgreich ist.  
  
 Eine synchrone Erkennungsvorgang kann aus den folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften.  
  
-   Das Erkennungsmodul Spracherkennung erkennt jedoch findet keine Übereinstimmungen in einer seiner geladen und aktiviert <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Um asynchrone Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und ein Erkennungsvorgang ausführt.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">Geben Sie das Zeitintervall, von denen eine von der Spracherkennung akzeptiert, enthält nur Ruhe vor dem Abschließen der Erkennung.</param>
        <summary>Führt eine synchrone Speech Recognition-Operation mit einem Punkt der angegebenen anfänglichen Ruhe-Timeout.</summary>
        <returns>Das Erkennungsergebnis für die Eingabe oder <see langword="null" /> , wenn der Vorgang nicht erfolgreich ist oder die Erkennung nicht aktiviert.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Spracherkennungsmoduls Spracherkennung innerhalb des durch angegebenen Zeitintervalls erkennt `initialSilenceTimeout` Argument <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> eine einzelne Recognition durchgeführt und wird dann beendet.  Die `initialSilenceTimeout` Parameter hat Vorrang vor der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaft.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
 Das Erkennungsmodul wird nicht ausgelöst. die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis bei Verwendung dieser Methode.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> Methode gibt ein <xref:System.Speech.Recognition.RecognitionResult> -Objekt, oder `null` , wenn der Vorgang nicht erfolgreich ist.  
  
 Eine synchrone Erkennungsvorgang kann aus den folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder für die `initialSilenceTimeout` Parameter.  
  
-   Das Erkennungsmodul Spracherkennung erkennt jedoch findet keine Übereinstimmungen in einer seiner geladen und aktiviert <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Um asynchrone Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und ein Erkennungsvorgang ausführt.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Startet einen asynchronen Speech Recognition-Vorgang.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methoden führen Sie einzelne oder mehrere asynchrone Erkennungsvorgänge. Das Erkennungsmodul führt jeder Vorgang für seine geladen und aktiviert Speech Recognition Grammatiken.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Wird ausgelöst, wenn ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen ist.  
  
 Um das Ergebnis eines asynchronen Recognition Vorgangs abzurufen, fügen Sie einen Ereignishandler an des Erkennungsmodul <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Das Erkennungsmodul löst dieses Ereignis, wenn eine synchrone oder asynchrone Erkennungsvorgang erfolgreich abgeschlossen wird. Wenn Recognition nicht erfolgreich war, die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> Eigenschaft auf <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> -Objekt, das Sie, in der Handler für zugreifen können die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis werden `null`.  
  
 Eine asynchrone Erkennungsvorgang kann aus den folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften.  
  
-   Das Erkennungsmodul Spracherkennung erkennt jedoch findet keine Übereinstimmungen in einer seiner geladen und aktiviert <xref:System.Speech.Recognition.Grammar> Objekte.  
  
-   Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> benötigen mindestens ein <xref:System.Speech.Recognition.Grammar> Objekt vor dem Ausführen von Recognition geladen. Um eine Sprache Recognition Grammatik zu laden, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Methode.  
  
-   Verwenden Sie zum Ändern, wie die Erkennung für die zeitliche Abfolge der Sprache oder Pausen in Bezug auf Erkennung behandelt, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
-   Um synchronen Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Führt ein einzelnes, asynchrones Spracherkennung Erkennungsvorgang an.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode führt ein einzelnes, asynchrones Erkennungsvorgang. Das Erkennungsmodul führt den Vorgang anhand seiner geladen und aktiviert Speech Recognition Grammatiken.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Wird ausgelöst, wenn ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen ist.  
  
 Um das Ergebnis eines asynchronen Recognition Vorgangs abzurufen, fügen Sie einen Ereignishandler an des Erkennungsmodul <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Das Erkennungsmodul löst dieses Ereignis, wenn eine synchrone oder asynchrone Erkennungsvorgang erfolgreich abgeschlossen wird. Wenn Recognition nicht erfolgreich war, die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> Eigenschaft auf <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> -Objekt, das Sie, in der Handler für zugreifen können die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis werden `null`.  
  
 Um synchronen Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende asynchrone Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und führt eine asynchrone Erkennungsvorgang. Ereignishandler sind angegeben, um zu veranschaulichen, die Ereignisse, die während des Vorgangs die Erkennung auslöst.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Gibt an, ob eine oder mehrere Erkennungsvorgänge durchführen.</param>
        <summary>Führt eine oder mehrere asynchrone Speech Recognition Vorgänge aus.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn `mode` ist <xref:System.Speech.Recognition.RecognizeMode.Multiple>, die Erkennung weiterhin Recognition asynchrone Vorgänge, bis die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> Methode wird aufgerufen.  
  
 Während eines Aufrufs an diese Methode kann die Erkennung der folgenden Ereignisse ausgelöst:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Wird ausgelöst, wenn die Erkennung Eingabe erkennt, die sie als Sprache leichter identifizieren kann.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Ausgelöst, wenn die Eingabe eine mehrdeutige Übereinstimmung mit einem aktiven Grammatiken erstellt.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Wird ausgelöst, wenn die Erkennung eines Erkennungsvorgangs schließt ab.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Wird ausgelöst, wenn ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Vorgang abgeschlossen ist.  
  
 Um das Ergebnis eines asynchronen Recognition Vorgangs abzurufen, fügen Sie einen Ereignishandler an des Erkennungsmodul <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Das Erkennungsmodul löst dieses Ereignis, wenn eine synchrone oder asynchrone Erkennungsvorgang erfolgreich abgeschlossen wird. Wenn Recognition nicht erfolgreich war, die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> Eigenschaft auf <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> -Objekt, das Sie, in der Handler für zugreifen können die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis werden `null`.  
  
 Eine asynchrone Erkennungsvorgang kann aus den folgenden Gründen fehlschlagen:  
  
-   Sprache wird nicht erkannt werden, bevor die Timeout-Intervalle für Ablaufen der <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> Eigenschaften.  
  
-   Das Erkennungsmodul Spracherkennung erkennt jedoch findet keine Übereinstimmungen in einer seiner geladen und aktiviert <xref:System.Speech.Recognition.Grammar> Objekte.  
  
 Um synchronen Erkennung auszuführen, gehen die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> Methoden.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende asynchrone Spracherkennung veranschaulicht. Das Beispiel erstellt eine <xref:System.Speech.Recognition.DictationGrammar>, lädt sie in einer in-Process-Spracherkennung und mehrere asynchrone Recognition Vorgänge ausführt. Die asynchronen Vorgänge sind nach 30 Sekunden abgebrochen. Ereignishandler sind angegeben, um zu veranschaulichen, die Ereignisse, die während des Vorgangs die Erkennung auslöst.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Beendet asynchrone Recognition ohne Wartezeiten für den aktuellen Erkennungsvorgang abgeschlossen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode schließt die asynchrone Recognition sofort ab. Wenn die aktuelle asynchrone Erkennungsvorgang Eingabe empfängt, die Eingabe ist abgeschnitten, und der Vorgang abgeschlossen ist, mit der Eingabe vorhandene. Die Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis aus, wenn ein asynchroner Vorgang wird abgebrochen, und legt sie fest der <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> auf `true`. Diese Methode bricht asynchrone Vorgänge gestartet, indem die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Verwenden, um asynchrone Recognition zu beenden, ohne das Abschneiden der Eingabe der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die Verwendung von veranschaulicht die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> Methode. Das Beispiel erstellt und lädt eine Spracherkennung Recognition-Grammatik, initiiert eine fortlaufende asynchrone Erkennungsvorgang und hält dann die 2 Sekunden, bevor sie den Vorgang abbricht. Das Erkennungsmodul empfängt Eingaben aus der Datei c:\temp\audioinput\sample.wav. Ereignishandler sind angegeben, um zu veranschaulichen, die Ereignisse, die während des Vorgangs die Erkennung auslöst.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Beendet die asynchrone Recognition nach Abschluss des aktuellen Recognition-Vorgangs.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Diese Methode schließt die asynchrone Recognition ohne Abschneiden Eingabe ab. Wenn die aktuelle asynchrone Erkennungsvorgang Eingabe empfängt, wird die Erkennung fortgesetzt, Eingaben angenommen werden, bis der aktuelle Recognition-Vorgang abgeschlossen ist. Die Erkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Ereignis aus, wenn ein asynchroner Vorgang wird beendet, und legt sie fest der <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> auf `true`. Diese Methode beendet die asynchrone Vorgänge gestartet, indem die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Verwenden, um sofort abzubrechen asynchronen Ansatz mit nur den vorhandenen Eingabe der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die die Verwendung von veranschaulicht die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> Methode. Das Beispiel erstellt und lädt eine Spracherkennung Recognition-Grammatik, initiiert eine fortlaufende asynchrone Erkennungsvorgang und hält dann die 2 Sekunden, bevor sie den Vorgang beendet. Das Erkennungsmodul empfängt Eingaben aus der Datei c:\temp\audioinput\sample.wav. Ereignishandler sind angegeben, um zu veranschaulichen, die Ereignisse, die während des Vorgangs die Erkennung auslöst.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> schließt einen asynchronen Erkennungsvorgang ab.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> des Objekts <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methode initiiert einen asynchronen Recognition-Vorgang. Wenn die Erkennung auf den asynchronen Vorgang abschließt, wird dieses Ereignis ausgelöst.  
  
 Der Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> -Ereignis, die Sie zugreifen können die <xref:System.Speech.Recognition.RecognitionResult> in die <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> Objekt. Wenn die Erkennung nicht erfolgreich war, <xref:System.Speech.Recognition.RecognitionResult> werden `null`. Um zu bestimmen, ob ein Timeout oder eine dienstunterbrechung Audioeingabe Recognition Fehler verursacht, können Sie die Eigenschaften für zugreifen <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, oder <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  
  
 Weitere Informationen finden Sie in den Ausführungen zur <xref:System.Speech.Recognition.RecognizeCompletedEventArgs>-Klasse.  
  
 Um die Details für die besten Kandidaten für die abgelehnte Recognition zu erhalten, fügen Sie einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignis.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel erkennt Ausdrücke wie z. B. "Zeigt eine Liste der Künstler in der Kategorie jazz" oder "Alben Gospel anzeigen". Im Beispiel wird einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> Ereignis, um Informationen zu den Ergebnissen der Erkennung in der Konsole angezeigt.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft den aktuellen Speicherort des der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> in das audio Eingabe, die er verarbeitet wird.</summary>
        <value>Die Position der Erkennung in der audio Eingabe, die ihn verarbeitet.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die audioposition bezieht sich auf jede von der Spracherkennung. Der Nullwert eines Eingabedatenstroms wird hergestellt, wenn diese Option aktiviert ist.  
  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Eigenschaftenverweise der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Position des Objekts in seine Audioeingabe. Im Gegensatz dazu, die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Eigenschaft verweist auf das Eingabegerät Position in der generierten Audiostream. Diese Positionen können unterschiedlich sein. Z. B. wenn die Erkennung erhalten hat Eingabe nicht für die It hat noch erzeugt ein Erkennungsergebnis wird der Wert der die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> -Eigenschaft muss kleiner als der Wert von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Eigenschaft.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Ruft Informationen zu der aktuellen Instanz der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Informationen zu den aktuellen von der Spracherkennung.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie zum Abrufen von Informationen für alle von der installierten Spracherkennung für das aktuelle System die <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> Methode.  
  
   
  
## Examples  
 Im folgenden Beispiel wird eine Teilliste der Daten für die aktuelle prozessintern Spracherkennungsmoduls. Weitere Informationen finden Sie unter <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn eine laufende <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> angehalten wird, um die Änderungen zu übernehmen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Anwendungen müssen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> anhalten eine laufende Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine> vor der Änderung der Einstellungen oder den zugehörigen <xref:System.Speech.Recognition.Grammar> Objekte. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst dieses Ereignis, wenn er Änderungen akzeptieren kann.  
  
 Während beispielsweise die <xref:System.Speech.Recognition.SpeechRecognitionEngine> wird angehalten, Sie können zu laden, entladen, aktivieren und deaktivieren <xref:System.Speech.Recognition.Grammar> Objekte aus, und ändern Sie die Werte für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaften. Weitere Informationen finden Sie unter der Methode <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel zeigt eine Konsolenanwendung, die geladen und entladen wird <xref:System.Speech.Recognition.Grammar> Objekte. Die Anwendung verwendet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode zum Anfordern der Spracherkennungsmoduls anhalten, damit sie ein Update empfangen kann. Die Anwendung dann lädt oder Entlädt eine <xref:System.Speech.Recognition.Grammar> Objekt.  
  
 Bei jeder Aktualisierung, einen Handler für <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis schreibt den Namen und den Status der derzeit geladenen <xref:System.Speech.Recognition.Grammar> Objekte in die Konsole. Wie Grammatiken geladen und entladen werden, erkennt die Anwendung zuerst die Namen der Farm Tieren auf die Namen von Tieren sowie die Namen der Früchte und dann nur die Namen von Früchten.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Fordert an, dass die Erkennung hält, um den Status zu aktualisieren.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Verwenden Sie diese Methode, um Änderungen an die Erkennung zu synchronisieren. Beispielsweise, wenn Sie laden oder Entladen von Spracherkennung Recognition Grammatik während die Erkennung Eingabe verarbeitet wird, verwenden Sie diese Methode und die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis, um das Verhalten Ihrer Anwendung mit dem Status der Erkennung zu synchronisieren.  
  
 Wenn diese Methode aufgerufen wird, die Erkennung hält oder asynchrone Vorgänge abgeschlossen und generiert eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis. Ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> -Ereignishandler können Sie den Status der Erkennung zwischen Erkennungsvorgänge ändern. Bei der Verarbeitung von <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignisse, die Erkennung angehalten, bis die Ereignishandler zurückgibt.  
  
> [!NOTE]
>  Wenn die Eingabe für die Erkennung geändert wird, bevor die Erkennung löst ist die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis, die Anforderung wird verworfen.  
  
 Wenn diese Methode aufgerufen wird:  
  
-   Die Erkennung nicht Eingabe verarbeitet, generiert die Erkennung sofort die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
-   Das Erkennungsmodul Eingabe verarbeitet, der Pausen oder Hintergrundgeräuschen besteht, wird die Erkennung hält den Erkennungsvorgang und generiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
-   Das Erkennungsmodul Eingabe verarbeitet, die nicht der stille oder Hintergrundgeräuschen besteht, wird die Erkennung Erkennungsvorgang abgeschlossen und generiert dann das <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis.  
  
 Während die Erkennung verarbeitet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis:  
  
-   Das Erkennungsmodul verarbeitet keine Eingabe, und der Wert von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Eigenschaft bleibt unverändert.  
  
-   Das Erkennungsmodul weiterhin erfassen, die Eingabe, und der Wert von der <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> -Eigenschaft ändern.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Fordert an, dass die Erkennung hält, um den Status zu aktualisieren.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Bei die Erkennung generiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> -Ereignis der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> ist `null`.  
  
 Um ein Benutzertoken bereitzustellen, verwenden Sie die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode. Verwenden Sie zum Angeben eines audioposition-Offsets der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt eine Konsolenanwendung, die geladen und entladen wird <xref:System.Speech.Recognition.Grammar> Objekte. Die Anwendung verwendet die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode zum Anfordern der Spracherkennungsmoduls anhalten, damit sie ein Update empfangen kann. Die Anwendung dann lädt oder Entlädt eine <xref:System.Speech.Recognition.Grammar> Objekt.  
  
 Bei jeder Aktualisierung, einen Handler für <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> Ereignis schreibt den Namen und den Status der derzeit geladenen <xref:System.Speech.Recognition.Grammar> Objekte in die Konsole. Wie Grammatiken geladen und entladen werden, erkennt die Anwendung zuerst die Namen der Farm Tieren auf die Namen von Tieren sowie die Namen der Früchte und dann nur die Namen von Früchten.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Benutzerdefinierte Informationen, die Informationen für den Vorgang enthält.</param>
        <summary>Fordert an, dass die Erkennung angehalten wird, um den Zustand aktualisieren und ein Benutzertoken für das zugeordnete Ereignis bietet.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Bei die Erkennung generiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> -Ereignis der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> enthält den Wert des der `userToken` Parameter.  
  
 Verwenden Sie zum Angeben eines audioposition-Offsets der <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Methode.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Benutzerdefinierte Informationen, die Informationen für den Vorgang enthält.</param>
        <param name="audioPositionAheadToRaiseUpdate">Der Offset aus dem aktuellen <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" /> um die Anforderung zu verzögern.</param>
        <summary>Fordert an, dass die Erkennung angehalten wird, um den Zustand aktualisieren und einen Offset und ein Benutzertoken für das zugeordnete Ereignis enthält.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Das Erkennungsmodul initiiert die updateanforderung für die Erkennung nicht erst der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> ist gleich den aktuellen <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> plus `audioPositionAheadToRaiseUpdate`.  
  
 Bei die Erkennung generiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> -Ereignis der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> Eigenschaft von der <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> enthält den Wert des der `userToken` Parameter.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">Der Eingabedatenstrom audio.</param>
        <param name="audioFormat">Das Format der Audioeingabe.</param>
        <summary>Konfiguriert die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Objekt, das über einen Audiostream Eingaben zu empfangen.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung während eines Erkennungsvorgangs das Ende des Eingabestreams erreicht ist, schließt der Erkennungsvorgang ab, mit der Eingabe zur Verfügung. Alle nachfolgenden Erkennungsvorgänge können eine Ausnahme generiert, es sei denn, Sie aktualisieren, dass die Eingabe für die Erkennung.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Im Beispiel wird die Eingabe aus einer Audiodatei example.wav, die die Ausdrücke enthält, "testen, testen eine zwei drei" und "Mister Cooper", die durch eine Pause getrennt. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Konfiguriert die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Objekt Eingabe von das Standardaudiogerät zu erhalten.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die grundlegende Spracherkennung veranschaulicht. Das Beispiel verwendet eine Ausgabe aus dem Standard-Audiogeräts, führt mehrere asynchrone Erkennungsvorgänge und wird beendet, wenn ein Benutzer den Ausdruck utters "exit".  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Deaktiviert die Eingabe für die von der Spracherkennung.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Konfigurieren der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Objekt für die keine Eingabe bei Verwendung der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden, oder wenn ein Erkennungsmodul vorübergehend offline dauert.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">Der Pfad der Datei, die als Eingabe verwendet.</param>
        <summary>Konfiguriert die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Objekt zum Empfangen von Eingaben aus einer Wellenform Audioformat (.wav)-Datei.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn während eines Erkennungsvorgangs das Ende der Eingabedatei für die Erkennung erreicht hat, schließt der Erkennungsvorgang ab, mit der Eingabe zur Verfügung. Alle nachfolgenden Erkennungsvorgänge können eine Ausnahme generiert, es sei denn, Sie aktualisieren, dass die Eingabe für die Erkennung.  
  
   
  
## Examples  
 Im folgenden Beispiel führt eine Erkennung für das Audio in eine WAV-Datei und den erkannten Text in die Konsole geschrieben.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">Der Datenstrom, die Audiodaten enthält.</param>
        <summary>Konfiguriert die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Objekt Eingabe aus einem Stream empfangen, die Wellenform Audioformat (.wav) Daten enthält.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung während eines Erkennungsvorgangs das Ende des Eingabestreams erreicht ist, schließt der Erkennungsvorgang ab, mit der Eingabe zur Verfügung. Alle nachfolgenden Erkennungsvorgänge können eine Ausnahme generiert, es sei denn, Sie aktualisieren, dass die Eingabe für die Erkennung.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> erkennt Eingabe, die sie als Sprache leichter identifizieren kann.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Jede von der Spracherkennung verfügt über einen Algorithmus, um die Unterscheidung zwischen Ruhe und Sprache. Wenn die <xref:System.Speech.Recognition.SpeechRecognitionEngine> führt ein Erkennungsvorgang Spracherkennung löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignis aus, wenn der Algorithmus die Eingabe als Sprache identifiziert. Die <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> Eigenschaft der zugeordneten <xref:System.Speech.Recognition.SpeechDetectedEventArgs> Objekt gibt an, Speicherort im Eingabedatenstrom, auf denen die Erkennung Spracherkennung erkannt. Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignis vor es keines löst der <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignisse.  
  
 Weitere Informationen finden Sie unter der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung für ein Flug Ursprungs- und Zielort Orte auswählen. Die Anwendung erkennt Ausdrücke an, wie z. B. "Ich möchte von Miami aus Chicago, fliegen."  Im Beispiel wird die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> Ereignis Bericht die <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> jedes Mal Sprache erkannt wird.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ein oder mehrere Wörter, die möglicherweise eine Komponente von mehreren vollständigen Ausdrücken in einer Grammatik erkannt wurde.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Die <xref:System.Speech.Recognition.SpeechRecognitionEngine> generiert zahlreiche <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> Ereignisse, wie er versucht, einen input-Ausdruck zu identifizieren. Sie erreichen den Text der teilweise erkannten Zeichenfolgen in die <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft von der <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> Objekt im Handler für die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> Ereignis. Diese Ereignisse behandeln wird in der Regel nur zum Debuggen nützlich.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> wird von <xref:System.Speech.Recognition.RecognitionEventArgs> abgeleitet.  
  
 Weitere Informationen finden Sie unter der <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft und die <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Methoden.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel erkennt Ausdrücke wie z. B. "Die Liste der Künstler in der Kategorie" jazz "anzeigen". Im Beispiel wird die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> Ereignis, um unvollständige Ausdruck Fragmente in der Konsole angezeigt, wie sie erkannt werden.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfängt Eingaben, die geladen und aktiviert entspricht keiner <see cref="T:System.Speech.Recognition.Grammar" /> Objekte.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Das Erkennungsmodul löst dieses Ereignis aus, wenn es feststellt, dass die Eingabe mit ausreichend geladen und aktiviert entspricht keiner <xref:System.Speech.Recognition.Grammar> Objekte. Die <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft von der <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> enthält die abgelehnte <xref:System.Speech.Recognition.RecognitionResult> Objekt. Können Sie den Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignis abzurufenden Recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> , die abgelehnt wurden und ihre <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> Bewertungen.  
  
 Wenn die Anwendung Parallelität mit einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz können Sie den Vertrauensgrad, welche Sprache Eingabe akzeptiert oder abgelehnt wird, mit einem der, Ändern der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden. Sie können ändern, wie die Spracherkennung auf nicht-Sprache Eingaben mithilfe reagiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Im folgende Beispiel erkennt Ausdrücke wie z. B. "Zeigt eine Liste der Künstler in der Kategorie jazz" oder "Alben Gospel anzeigen". Im Beispiel wird einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> Ereignis, um eine Benachrichtigung in der Konsole angezeigt, wenn die Spracherkennung Eingabe kann nicht zugeordnet werden, um den Inhalt der Grammatik mit ausreichenden <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> um einen erfolgreichen Erkennung zu erzeugen. Der Ereignishandler zeigt auch die Anerkennung Ergebnis <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> , die aufgrund von wenig vertrauensergebnisse abgelehnt wurden.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Wird ausgelöst, wenn die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> empfängt Eingaben, die alle geladen und aktiviert des entspricht <see cref="T:System.Speech.Recognition.Grammar" /> Objekte.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Sie können eine mit einer der Erkennungsvorgang Initiieren der <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> oder <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Methoden. Das Erkennungsmodul löst die <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> -Ereignis, wenn es feststellt, dass die Eingabe mindesten einem der geladenen <xref:System.Speech.Recognition.Grammar> Objekte mit einen ausreichenden Grad an Vertrauen Recognition bilden. Die <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> Eigenschaft von der <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> enthält die akzeptierte <xref:System.Speech.Recognition.RecognitionResult> Objekt. Handler des <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignisse erhalten dem erkannten Ausdruck als auch eine Liste der Recognition <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> mit niedrigeren vertrauensergebnisse.  
  
 Wenn die Anwendung Parallelität mit einer <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz können Sie den Vertrauensgrad, welche Sprache Eingabe akzeptiert oder abgelehnt wird, mit einem der, Ändern der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden.  Sie können ändern, wie die Spracherkennung auf nicht-Sprache Eingaben mithilfe reagiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 Wenn die Erkennung Eingabe erhält, die eine Grammatik entspricht der <xref:System.Speech.Recognition.Grammar> Objekt auslösen kann seine <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis. Die <xref:System.Speech.Recognition.Grammar> des Objekts <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis wird ausgelöst, bevor der Spracherkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Alle Aufgaben, die spezifisch für eine bestimmte Grammatik sollten immer ausgeführt werden, von einem Handler für das <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Ereignis.  
  
 Beim Erstellen eines <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>-Delegaten bestimmen Sie die Methode für die Ereignisbehandlung. Um dem Ereignishandler das Ereignis zuzuordnen, fügen Sie dem Ereignis eine Instanz des Delegaten hinzu. Der Ereignishandler wird bei jedem Eintreten des Ereignisses aufgerufen, sofern der Delegat nicht entfernt wird. Weitere Informationen über Delegaten für Ereignishandler finden Sie unter [Ereignissen und Delegaten](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Das folgende Beispiel ist Teil einer Konsolenanwendung, die Spracherkennung Recognition Grammatik Konstrukte erstellt eine <xref:System.Speech.Recognition.Grammar> Objekt, und lädt sie in der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Recognition ausführen. Im Beispiel wird veranschaulicht, Spracheingabe für eine <xref:System.Speech.Recognition.SpeechRecognitionEngine>, die zugeordneten Erkennungsergebnisse und die zugeordneten Ereignissen, die von der von der Spracherkennung ausgelöst.  
  
 Eingabe gesprochen, z. B. "Ich möchte von Chicago nach Miami fliegen" ausgelöst werden ein <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis. Sprechen den Ausdruck "Fliegen me aus Houston nach Chicago" wird nicht ausgelöst, eine <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis.  
  
 Im Beispiel wird einen Handler für das <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> Ereignis anzuzeigenden erfolgreich erkannt wird, Ausdrücke und die Semantik, die sie in der Konsole enthalten.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Entfernt alle <see cref="T:System.Speech.Recognition.Grammar" /> Objekte aus der Erkennung.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn zurzeit die Erkennung lädt einer <xref:System.Speech.Recognition.Grammar> asynchron ausgeführt wird, diese Methode wartet, bis die <xref:System.Speech.Recognition.Grammar> geladen wird, bevor alle Entladen der <xref:System.Speech.Recognition.Grammar> Objekte aus der <xref:System.Speech.Recognition.SpeechRecognitionEngine> Instanz.  
  
 Verwenden Sie zum Entladen einer bestimmten Grammatik der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die das synchrone laden und Entladen von Spracherkennung Recognition Grammatiken veranschaulicht.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Das grammatikobjekt, entladen werden soll.</param>
        <summary>Entfernt ein angegebenes <see cref="T:System.Speech.Recognition.Grammar" /> -Objekt aus der <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Instanz.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Wenn die Erkennung ausgeführt wird, müssen die Anwendungen verwenden <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Anhalten der <xref:System.Speech.Recognition.SpeechRecognitionEngine> -Instanz vor dem Laden, entladen, aktivieren oder Deaktivieren einer <xref:System.Speech.Recognition.Grammar> Objekt. Alle entladen <xref:System.Speech.Recognition.Grammar> Objekte, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> Methode.  
  
   
  
## Examples  
 Das folgende Beispiel zeigt einen Teil einer Konsolenanwendung, die das synchrone laden und Entladen von Spracherkennung Recognition Grammatiken veranschaulicht.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">Die Grammatik wurde nicht in dieses Erkennungsmodul geladen, oder diese Erkennung ist zurzeit lädt die Grammatik asynchron.</exception>
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Der Wert einer Einstellung aktualisiert für die Erkennung.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Erkennung Einstellungen können Zeichenfolge, 64-Bit-Ganzzahl oder Arbeitsspeicher-Adressdaten enthalten. Die folgende Tabelle beschreibt die Einstellungen, die für eine Microsoft-Speech-API (SAPI) definiert sind – Erkennung kompatibel. Die folgenden Einstellungen benötigen denselben Bereich für jeden Erkennungsmodul, die die Einstellung unterstützt. Eine SAPI-kompatible Erkennungsmodul ist nicht erforderlich, diese Einstellungen unterstützt und kann andere Einstellungen unterstützen.  
  
|Name|Beschreibung|  
|----------|-----------------|  
|`ResourceUsage`|Gibt die Erkennung CPU-Auslastung an. Der Bereich liegt zwischen 0 und 100. Der Standardwert ist 50.|  
|`ResponseSpeed`|Gibt die Länge der Pause am Ende der Eingabe eindeutig an, bevor die von der Spracherkennung ein Erkennungsvorgangs abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 Millisekunden (ms). Diese Einstellung bezieht sich auf der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> Eigenschaft. Standard = 150ms.|  
|`ComplexResponseSpeed`|Gibt die Länge der Pause in Millisekunden (ms) am Ende der mehrdeutigen Eingaben an, bevor die von der Spracherkennung ein Erkennungsvorgangs abgeschlossen ist. Der Bereich liegt zwischen 0 und 10.000 ms. Diese Einstellung bezieht sich auf der Erkennung <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaft. Standard = 500 ms.|  
|`AdaptationOn`|Gibt an, ob die Anpassung des Modells acoustic auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`). Der Standardwert ist `1` (ON).|  
|`PersistedBackgroundAdaptation`|Gibt an, ob die Anpassung im Hintergrund auf ON festgelegt ist (Wert = `1`) oder OFF (Wert = `0`), und speichert die Einstellung in der Registrierung. Der Standardwert ist `1` (ON).|  
  
 Um eine der Einstellungen für die Erkennung zurückzugeben, verwenden die <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> Methode.  
  
 Mit Ausnahme von `PersistedBackgroundAdaptation`, Eigenschaftswerte, die festlegen, mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methoden behalten ihre Gültigkeit nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>, nach denen sie auf die Standardeinstellungen zurückgesetzt.  
  
 Sie können ändern, wie die Spracherkennung auf nicht-Sprache Eingaben mithilfe reagiert die <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, und <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> Eigenschaften.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der Einstellung aktualisiert.</param>
        <param name="updatedValue">Der neue Wert für die Einstellung.</param>
        <summary>Aktualisiert die angegebene Einstellung für die <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> mit den angegebenen ganzzahligen Wert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Mit Ausnahme von `PersistedBackgroundAdaptation`, Eigenschaftswerte, die festlegen, mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methode behalten ihre Gültigkeit nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>, nach denen sie auf die Standardeinstellungen zurückgesetzt. Finden Sie unter <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Beschreibungen der unterstützten Einstellungen.  
  
   
  
## Examples  
 Im folgende Beispiel ist Teil einer Konsolenanwendung, die die Werte für eine Reihe von den Einstellungen für die Erkennung, die das Gebietsschema En-US unterstützt definierten ausgibt. Im Beispiel aktualisiert die Einstellungen für die Zuverlässigkeit und fragt dann die Erkennung, um die aktualisierten Werte zu überprüfen. Im Beispiel wird die folgende Ausgabe generiert.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Das Erkennungsmodul verfügt nicht über für eine Einstellung mit diesem Namen.</exception>
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Der Name der Einstellung aktualisiert.</param>
        <param name="updatedValue">Der neue Wert für die Einstellung.</param>
        <summary>Aktualisiert die angegebene Sprache Recognition Engine-Einstellung mit dem angegebenen Zeichenfolgenwert.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Mit Ausnahme von `PersistedBackgroundAdaptation`, Eigenschaftswerte, die festlegen, mit der <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Methode behalten ihre Gültigkeit nur für die aktuelle Instanz von <xref:System.Speech.Recognition.SpeechRecognitionEngine>, nach denen sie auf die Standardeinstellungen zurückgesetzt. Finden Sie unter <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> Beschreibungen der unterstützten Einstellungen.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> ist <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> ist die leere Zeichenfolge ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Das Erkennungsmodul verfügt nicht über für eine Einstellung mit diesem Namen.</exception>
      </Docs>
    </Member>
  </Members>
</Type>
